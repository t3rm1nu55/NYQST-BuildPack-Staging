# Superagent Agentic System Analysis

> **Date**: 2026-02-16
> **Status**: Comprehensive analysis based on bundle extraction, streaming protocol analysis, and UI observation
> **Confidence Levels**: HIGH (directly observed), MEDIUM (inferred from patterns), LOW (speculative)

---

## Executive Summary

Superagent orchestrates AI agent workflows through a hierarchical planning system that decomposes user prompts into parallel research tasks, executes them via specialized agents with tool access, synthesizes results, and generates deliverables. The system uses event-sourced execution with NDJSON streaming, supports human-in-the-loop interrupts, and maintains durable replay capabilities.

**Key Architecture Pattern**: Planner → Fan-out Executors → Synthesizer → Deliverable Generator

---

## A. Agent Catalog

### A.1 Primary Orchestration Agents

| Agent Type | Role | Evidence | Confidence |
|------------|------|----------|------------|
| **Planner** | Decomposes user prompt into `PlanSet` containing ordered `Plans` and `PlanTasks` | Streaming event `task_update` with `plan_set` payload; screenshot showing 13 parallel workstreams | HIGH |
| **Research Executor** | Executes individual research tasks using web search and scrape tools | Screenshot showing task cards with states: `PROCESSING`, `CREATING NOTES`; activity panel showing "Deep Market Landscape Analysis", "Comprehensive Financial Analysis", etc. | HIGH |
| **Synthesizer** | Aggregates results from parallel executors into structured content | Screenshot activity panel: "Considering Results and Refining Research Strategy"; generation pipeline evidence of data brief feeding all generators | MEDIUM |
| **Deliverable Generator** | Produces final output (report/website/slides/doc) with citations | v0.app integration evidence; report export with "Generated by Superagent" footer; metadata `generator: 'v0.app'` | HIGH |

### A.2 Specialized Sub-Agents

| Agent Type | Role | Evidence | Confidence |
|------------|------|----------|------------|
| **Meta-Reasoning Agent** | Evaluates research progress and refines strategy | Screenshot: "Considering Results and Refining Research Strategy" section showing FactSet gap-filling logic | MEDIUM-HIGH |
| **Standards Agent** | (Report generation) Creates design conventions and chart styles | Report A git history: "Phase 1: Standards agent creates chart-styles.ts, conventions doc" | HIGH |
| **Feature Branch Agents** | (Report generation) Parallel visualization builders (A, B, C, D) | Report A git history: 4 agents on feature branches building different chart types in ~100 seconds each | HIGH |
| **Integration Agent** | (Report generation) Merges parallel work, fixes type errors, validates build | Report A git history: "Phase 3: Integration agent merges, fixes type errors" | HIGH |

### A.3 Agent Identification in Streaming Protocol

**Confidence**: MEDIUM — Inferred from event schema, not directly observed in runtime logs.

Agents are identified through hierarchical IDs in streaming events:
- `plan_set_id` — Top-level execution context (maps to user message/run)
- `plan_id` — Phase/subgraph identifier within plan set
- `node_id` — Individual agent/executor within a plan
- `tool_id` — Specific tool invocation within a node

**Event Evidence**:
```typescript
// From Zod schema in 774-e1971e2500ea1c79.js
type NodeToolEvent = {
  type: "node_tool_event";
  plan_set_id: string;
  plan_id: string;
  node_id: string;
  tool_id: string;
  event: string;
  metadata?: Record<string, unknown>;
  timestamp: number;
}
```

---

## B. Orchestration Mechanics

### B.1 Plan Decomposition Process

**Confidence**: HIGH for data structures, MEDIUM-HIGH for execution flow.

#### Data Model

From Zod schemas and TypeScript interfaces observed in bundles:

```typescript
// Plan hierarchy (linked-list structure)
type PlanSet = {
  plans: Record<string, Plan>;
  chat_id: string;
  workspace_id: string;
}

type Plan = {
  plan_id: string;
  plan_tasks: Record<string, PlanTask>;
  previous_plan_id: string | null;  // Linked list ordering
  status: Status;  // "loading" | "success" | "error"
  used_sources: Source[];
}

type PlanTask = {
  plan_task_id: string;
  title: string;
  message: string;
  previous_task_id: string | null;  // Linked list ordering
  status: Status;
}
```

#### Observed Plan Example

From screenshot analysis (Activity panel at 03:48:18):

**Plan: "Devising Initial Research Strategy"**

13 parallel workstreams created:
1. Extract and Analyze Previous Work
2. Deep Market Landscape Analysis
3. Comprehensive Financial Analysis
4. Strategic Initiatives Research
5. Deep Customer Insights Research
6. Regulatory Environment Analysis
7. Technology and Innovation Trends
8. Supply Chain and Operations Research
9. ESG and Sustainability Analysis
10. Comprehensive Risk Analysis
11. Geographic Market Analysis
12. Investor Sentiment and Market Perception
13. Design Excellence Research
14. Academic Research and Insights *(Note: 14 total, not 13 as initially estimated)*

**Follow-up Plan: "Considering Results and Refining Research Strategy"**

> "Fill critical data gaps from FactSet failures by gathering financial, geographic, and investor data from alternative sources (SEC filings, Balance...)"

This shows **meta-reasoning** and **adaptive re-planning** based on tool failures.

### B.2 Parallel Dispatch Mechanism

**Confidence**: MEDIUM-HIGH (strong pattern evidence, no direct code observation).

#### Evidence for Fan-Out Pattern

1. **Screenshot**: Task cards 20-23 running simultaneously at 03:31:40, all showing `PROCESSING` status with independent timers (0:19, 0:19, 0:15, 0:37).

2. **Streaming Event Design**: `node_tools_execution_start` event includes:
   ```typescript
   {
     type: "node_tools_execution_start";
     plan_id: string;
     node_id: string;
     tool_ids: string[];      // Batch of tools
     total_tools: number;
     timestamp: number;
   }
   ```
   This indicates batch scheduling of multiple tools/nodes in parallel.

3. **Report Generation Evidence**: Report A git history shows 4 agents working simultaneously on feature branches, completing in ~100 seconds each.

#### Inferred Implementation Pattern

Most likely pattern (aligns with LangGraph `Send()` API):

```python
# Pseudocode based on observed behavior
def planner_node(state):
    # LLM generates plan_set with structured output
    plan_set = llm_generate_plan(user_prompt)

    # Emit task_update event
    emit_event("task_update", plan_set=plan_set, status="loading")

    # Fan-out: Create parallel subgraph invocations
    return [
        Send("research_executor", {"task": task})
        for task in plan_set.all_tasks()
    ]

def research_executor(state):
    # Individual research agent with tool loop
    task = state["task"]
    emit_event("update_subagent_current_action",
               current_action=f"Searching for {task.title}")

    # Tool execution loop
    results = []
    for tool in task.required_tools:
        result = execute_tool(tool)
        results.append(result)
        emit_event("node_tool_event", tool_id=tool.id, event="completed")

    # Synthesis
    notes = synthesize_results(results)
    emit_event("update_subagent_current_action",
               current_action="Creating notes")

    return {"task_id": task.id, "notes": notes, "sources": results}
```

**Terminology Mapping**:

| Superagent Concept | LangGraph Equivalent |
|--------------------|---------------------|
| `plan_set_id` | Graph thread ID / run ID |
| `plan_id` | Subgraph / phase ID |
| `node_id` | Graph node ID / step ID |
| `Send()` pattern | Parallel dispatch (exact match) |

### B.3 Task Lifecycle State Machine

**Confidence**: HIGH (directly observed in screenshots and schemas).

#### States Observed

From screenshot task cards and streaming event schema:

```
LOADING → PROCESSING → CREATING NOTES → SUCCESS
                    ↓
                  ERROR
```

**State Definitions**:

| State | Meaning | UI Indicator | Evidence |
|-------|---------|--------------|----------|
| `LOADING` | Task queued, not started | — | Schema `Status = "loading" \| "success" \| "error"` |
| `PROCESSING` | Executing research/tools | Task card badge: `PROCESSING` | Screenshot 03:31:40, tasks 20-22 |
| `CREATING NOTES` | Synthesizing results | Task card badge: `CREATING NOTES` | Screenshot 03:31:40, task 23 |
| `SUCCESS` | Completed successfully | Task card complete | Schema status value |
| `ERROR` | Failed execution | Task card error state | Schema status value |

#### Event Transitions

```typescript
// Lifecycle events (inferred from schema + screenshots)
1. task_update(status: "loading", plan_set: {...})        // Task created
2. node_tools_execution_start(tool_ids: [...])             // Tools starting
3. update_subagent_current_action(current_action: "...")   // Progress updates
4. node_tool_event(event: "tool_call_started", ...)       // Per-tool events
5. node_tool_event(event: "tool_call_completed", ...)     // Tool finished
6. update_subagent_current_action(current_action: "Creating notes")
7. task_update(status: "success", plan_set: {...})        // Task done
```

### B.4 Fan-In Aggregation

**Confidence**: MEDIUM (inferred from generation pipeline analysis).

#### Evidence

From `GENERATION_PIPELINE.md`:

> "Report C contains 5-year financial time series for AAPL/MSFT/GOOG across revenue, net income, operating cash flow... These figures are internally consistent across sections and widgets (e.g., MSFT FY2025 revenue of $281.7B appears identically in competitive-matrix, dynamic-financial-chart, financial-performance prose, and executive-summary)."

**Inference**: An aggregation process creates a unified **data brief** from all parallel research results. This brief is then passed to all downstream generators to ensure consistency.

#### Likely Pattern

```python
def fan_in_aggregation(state):
    # Collect all research results
    all_results = state["parallel_results"]

    # Combine into structured brief
    data_brief = {
        "financial_data": merge_financial_results(all_results),
        "market_data": merge_market_results(all_results),
        "citations": collect_all_sources(all_results),
        # ... other sections
    }

    # Store as artifact
    store_artifact("data_brief", data_brief)

    return {"data_brief": data_brief}
```

### B.5 Meta-Reasoning Process

**Confidence**: MEDIUM-HIGH (observed in screenshot activity panel).

#### Evidence

Screenshot 03:48:18 shows two distinct reasoning phases:

**Phase 1**: "Devising Initial Research Strategy"
- Creates 14 parallel workstreams
- Initiates broad research sweep

**Phase 2**: "Considering Results and Refining Research Strategy"
- Analyzes results from Phase 1
- Identifies gaps (FactSet failures)
- Dispatches targeted gap-filling research

**Pattern**: Observe → Reason → Adapt → Re-dispatch

This suggests a **multi-stage orchestration** where the planner can:
1. Create initial research plan
2. Wait for results
3. Evaluate completeness/quality
4. Generate follow-up plan to address gaps

#### Likely Implementation

```python
# Two-stage graph with conditional branching
def meta_planner_node(state):
    if state["iteration"] == 0:
        # Initial broad research
        return create_broad_plan(state)
    else:
        # Evaluate previous results
        gaps = analyze_research_gaps(state["results"])
        if gaps:
            # Create targeted follow-up plan
            return create_gap_filling_plan(gaps)
        else:
            # Proceed to synthesis
            return Continue("synthesizer")
```

---

## C. Tool Registry

**Confidence**: MEDIUM-LOW for specific tools (inferred from public information and naming patterns).

### C.1 Observed Tool Categories

From screenshot analysis, Airtable newsroom announcement, and bundle analysis:

| Category | Tools (Inferred) | Evidence Source |
|----------|-----------------|-----------------|
| **Web Search** | Brave Search | Airtable newsroom: "premium sources"; standard web search API |
| **Web Scraping** | Firecrawl / Jina Reader | Common scraping patterns for research agents |
| **Financial Data** | FactSet, Polygon, FRED | Screenshot gap-filling reference; financial data providers |
| **Company Data** | Crunchbase | Airtable newsroom mention: "Crunchbase" |
| **Earnings/Transcripts** | Quartr | Airtable newsroom mention: "transcripts" |
| **SEC Filings** | Financial Documents API | Screenshot: "SEC filings" as fallback for FactSet |
| **Market Data** | Polygon (stocks/crypto) | Common financial data provider |
| **Prediction Markets** | Kalshi | Emerging data source for market sentiment |
| **Alt Data** | QuiverQuant | Political/insider trading data |

**Note**: Specific tool names are partially confirmed from Airtable's public announcement and partially inferred from common research agent tooling patterns.

### C.2 Tool Return Schemas

**Confidence**: LOW (not directly observed in bundles).

**Inferred Pattern** (based on citation/source schemas):

```typescript
type ToolResult = {
  tool_id: string;
  tool_type: "WEB" | "DOCUMENT" | "FINANCIAL_DATA" | "API";
  content: string;           // Markdown or JSON
  metadata: {
    source_url?: string;
    title?: string;
    author?: string;
    published_date?: string;
    relevance_score?: number;
  };
  entities?: Entity[];       // Extracted entities/citations
}
```

### C.3 Tool Execution Flow

**Confidence**: MEDIUM-HIGH (inferred from event schema and screenshot evidence).

```
1. Planner assigns tools to nodes
   ↓
2. node_tools_execution_start emitted (batch accounting)
   ↓
3. For each tool:
   a. node_tool_event(event: "tool_call_started")
   b. Execute tool (external API call)
   c. Store result as Entity/Citation
   d. node_tool_event(event: "tool_call_completed", metadata: {...})
   ↓
4. update_subagent_current_action("Creating notes from N sources")
   ↓
5. Synthesize tool results into structured notes
   ↓
6. Return to fan-in aggregation
```

### C.4 Tool Failure Handling

**Confidence**: MEDIUM (observed in screenshot meta-reasoning).

**Evidence**: Screenshot shows FactSet failures triggering fallback strategy:

> "Fill critical data gaps from FactSet failures by gathering financial, geographic, and investor data from alternative sources (SEC filings, Balance...)"

**Pattern**: Tool-level retries with fallback to alternative providers.

```python
def execute_tool_with_fallback(tool_spec, retries=3):
    primary_tool = tool_spec.primary
    fallback_tools = tool_spec.fallbacks

    for attempt in range(retries):
        try:
            result = call_tool(primary_tool)
            if result.success:
                return result
        except ToolError:
            if attempt == retries - 1:
                break

    # Fallback cascade
    for fallback in fallback_tools:
        try:
            result = call_tool(fallback)
            if result.success:
                emit_event("node_tool_event",
                          event="fallback_used",
                          metadata={"fallback": fallback.name})
                return result
        except ToolError:
            continue

    # All failed → trigger meta-reasoning
    emit_event("node_tool_event", event="all_tools_failed")
    raise ToolFailureError()
```

---

## D. State Flow Between Agents

**Confidence**: MEDIUM (inferred from architecture analysis).

### D.1 Data Flow Diagram

```
┌──────────────┐
│ User Prompt  │
└──────┬───────┘
       ↓
┌──────────────────────────────────────┐
│ Planner Agent                        │
│ • LLM structured output → PlanSet    │
│ • Emits: task_update(plan_set, ...)  │
└──────┬───────────────────────────────┘
       ↓ (Send() fan-out)
┌─────────────────────────────────────────────────┐
│ Research Executors (Parallel)                   │
│ Node 1, 2, 3, ... N                              │
│ • Web search → scraped content                  │
│ • Financial APIs → structured data              │
│ • Emits: node_tool_event, current_action        │
│ • Output: {notes, sources, entities}            │
└──────┬──────────────────────────────────────────┘
       ↓ (Collect all)
┌──────────────────────────────────┐
│ Fan-In Aggregator                │
│ • Merge all research results     │
│ • Create unified data brief      │
│ • Cross-reference citations      │
│ • Validate consistency           │
└──────┬───────────────────────────┘
       ↓
┌──────────────────────────────────┐
│ Synthesizer Agent                │
│ • LLM synthesis with data brief  │
│ • Structured content generation  │
│ • Citation embedding             │
└──────┬───────────────────────────┘
       ↓
┌──────────────────────────────────┐
│ Deliverable Generator (v0.app)   │
│ • Report: Multi-agent TSX gen    │
│ • Website: Template + widgets    │
│ • Slides/Docs: (inferred)        │
│ • Emits: entity.created events   │
└──────┬───────────────────────────┘
       ↓
┌──────────────────────────────────┐
│ Artifact Store + Export          │
│ • ZIP export (atomic timestamp)  │
│ • Durable entity storage         │
└──────────────────────────────────┘
```

### D.2 State Persistence

**Confidence**: HIGH (direct evidence from replay functionality).

From `02_streaming_protocol.md`:

> "ReplayStream mode: The same module contains a replay implementation... iterates over a stored list of prior events, calls the same onEvent handler, and synthesizes a terminal done with the stored final message."

**Implication**: All execution state is **event-sourced** — the complete run can be reconstructed from the event log.

**Storage Model** (inferred):

```typescript
// Per-run event log (append-only)
type RunLog = {
  run_id: string;
  message_stream_id: string;
  events: StreamEvent[];  // Ordered by timestamp
}

// Materialized state (for queries)
type RunState = {
  run_id: string;
  plan_set: PlanSet;
  current_phase: "planning" | "research" | "synthesis" | "generation";
  completed_tasks: Set<string>;
  artifacts: Entity[];
  status: "running" | "completed" | "error";
}
```

### D.3 Intermediate Result Storage

**Confidence**: MEDIUM-HIGH (inferred from entity abstraction).

From streaming event schema:

```typescript
type Entity = {
  entity_id: string;
  entity_type: "citation" | "artifact" | "report" | "website" | "document";
  content?: string;
  metadata?: Record<string, unknown>;
}
```

**Pattern**: Each tool result → stored as Entity → referenced by ID in subsequent stages.

**Benefits**:
1. **Deduplication**: Same source scraped once, referenced multiple times
2. **Incremental generation**: Artifacts can be previewed before completion
3. **Citation tracking**: Full provenance from source → insight → final output

### D.4 Error Propagation

**Confidence**: MEDIUM (inferred from event schema and meta-reasoning evidence).

#### Error Levels

| Level | Scope | Handling | Evidence |
|-------|-------|----------|----------|
| **Tool Error** | Single tool call fails | Retry → Fallback → Log metadata | FactSet failure example |
| **Task Error** | Entire task/node fails | Mark task status="error", continue others | Schema supports per-task error status |
| **Plan Error** | Critical failure blocks progress | Trigger clarification_needed or ERROR event | Schema includes clarification_needed event |
| **Run Error** | Fatal orchestration failure | Terminal ERROR event, halt stream | Schema includes ERROR event type |

#### Graceful Degradation Pattern

From screenshot evidence of gap-filling:

```
1. Execute primary plan
2. Identify failed/incomplete tasks
3. Meta-reasoning: analyze gaps
4. Create targeted follow-up plan
5. Execute gap-filling research
6. Proceed to synthesis with best available data
```

**Key Insight**: System prioritizes **completing with partial data** over halting on errors.

---

## E. Streaming Event Protocol

### E.1 Complete Event Taxonomy

**Confidence**: HIGH (directly extracted from Zod schemas).

From `774-e1971e2500ea1c79.js` — 22 event types:

#### Lifecycle Events
| Event | Purpose | Payload |
|-------|---------|---------|
| `stream_start` | Initialize stream | `{chat_id, creator_user_id, user_chat_message_id, workspace_id}` |
| `heartbeat` | Keep-alive ping | `{}` |
| `done` | Finalize stream | `{message?, has_async_entities_pending?: bool}` |
| `ERROR` | Stream error | `{error_type: string, error_message: string}` |

#### Message/Content Events
| Event | Purpose | Payload |
|-------|---------|---------|
| `message_delta` | Incremental text tokens | `{delta: string}` |
| `ai_message` | Structured message | `{message: Message}` |
| `message_is_answer` | Mark as answer | `{is_answer: boolean}` |
| `chat_title_generated` | Auto-generate title | `{title: string}` |
| `clarification_needed` | Request user input | `{message: Message}` |
| `update_message_clarification_message` | Update clarification | `{update: {chat_message_id, needs_clarification_message}}` |

#### Planning/Execution Events
| Event | Purpose | Payload |
|-------|---------|---------|
| `task_update` | Task status change | `{key, message, plan_set: PlanSet, status: Status, title, metadata?}` |
| `pending_sources` | List pending tasks | `{pending_sources: PendingSource[]}` |
| `node_tools_execution_start` | Batch tool start | `{node_id, plan_id, plan_set_id, tool_ids: string[], total_tools, timestamp}` |
| `node_tool_event` | Per-tool telemetry | `{event, metadata?, node_id, plan_id, plan_set_id, tool_id?, timestamp}` |
| `update_subagent_current_action` | Progress string | `{current_action, node_id, plan_id, plan_set_id, timestamp?}` |

#### Report/Preview Events
| Event | Purpose | Payload |
|-------|---------|---------|
| `node_report_preview_start` | Begin preview | `{entity, final_report, node_id, plan_id, plan_set_id, preview_id, report_title, report_user_query, section_id?}` |
| `node_report_preview_delta` | Incremental preview | `{delta, node_id, plan_id, plan_set_id, preview_id, section_id?}` |
| `node_report_preview_done` | Final preview | `{content, entity?, final_report, node_id, plan_id, plan_set_id, preview_id, report_title, report_user_query, section_id?}` |

#### Citation/Entity Events
| Event | Purpose | Payload |
|-------|---------|---------|
| `references_found` | Attach citations | `{references: Entity[]}` |

#### Browser-Use (Human-in-Loop) Events
| Event | Purpose | Payload |
|-------|---------|---------|
| `browser_use_start` | Open browser session | `{browser_session_id, browser_stream_url, timestamp}` |
| `browser_use_stop` | Close session | `{browser_session_id}` |
| `browser_use_await_user_input` | Pause for input | `{browser_session_id, agent_message?: string}` |

### E.2 Event Ordering Guarantees

**Confidence**: MEDIUM-HIGH (inferred from protocol design).

#### Temporal Ordering
- All events include `timestamp: number` (epoch milliseconds)
- Client processes events in arrival order
- No explicit sequence numbers (relies on transport order)

#### Causal Ordering
Strong causal relationships observed:

```
stream_start → (task_update | message_delta | ...) → done/ERROR/clarification_needed
                                                      ↑ (exactly one terminal event)

task_update(status: "loading")
  → node_tools_execution_start
    → node_tool_event (multiple)
      → update_subagent_current_action (multiple)
        → task_update(status: "success")

node_report_preview_start
  → node_report_preview_delta (multiple)
    → node_report_preview_done
```

#### No Strict Ordering Between Parallel Nodes
- Multiple `update_subagent_current_action` events for different `node_id`s can be interleaved
- Client must track state per `(plan_set_id, plan_id, node_id)` tuple

### E.3 Event-to-UI Mapping

**Confidence**: HIGH (observed in screenshot and documented in bundle analysis).

| Event | UI Component | Update Type |
|-------|-------------|-------------|
| `task_update` | Task card (left panel) | Create/update card with status badge, progress bar |
| `update_subagent_current_action` | Task card secondary text | Update "what I'm doing" string |
| `node_tools_execution_start` | Task card progress | Initialize progress bar (0% → N tools) |
| `node_tool_event` | Task card progress | Increment progress (M/N tools complete) |
| `message_delta` | Chat message area | Append token to streaming message |
| `references_found` | Citation pills (below message) | Render source pills with click handlers |
| `node_report_preview_start` | Generation overlay | Show "Generating output..." modal |
| `node_report_preview_delta` | Generation substep text | Update "Building components..." / "Polishing..." |
| `done` | Full UI | Dismiss overlay, mark message complete |

#### Citation Buffering

From `02_streaming_protocol.md`:

> "Citation buffering during token streaming: The module includes a small state machine that delays rendering message_delta output while inside square brackets... does not advance the 'ready pointer' until it sees ] (or newline) to close the citation."

**Pattern**: Client buffers incomplete citations (`[1`, `[12`) to avoid flickering partial numbers.

### E.4 Stream Transport Details

**Confidence**: HIGH (directly observed in bundle code).

#### Request
```
GET /api/chat/message/stream?message_stream_id={id}

Headers:
  Connection: keep-alive
  Cache-Control: no-cache
  X-Gradient-Browser-Client: "1"
  X-Gradient-Workspace-Id: {workspace_id}
  Credentials: include  (cookie auth)
```

#### Response
```
Content-Type: application/x-ndjson
Transfer-Encoding: chunked

{"data":{"type":"stream_start","chat_id":"..."},"timestamp":1737849600000}\n
{"data":{"type":"heartbeat"},"timestamp":1737849601000}\n
{"data":{"type":"task_update","plan_set":{...},"status":"loading"},"timestamp":1737849602000}\n
...
```

#### Client-Side Processing

From `774-e1971e2500ea1c79.js`:

```typescript
const reader = response.body.getReader();
const decoder = new TextDecoder();

let buffer = "";
while (true) {
  const {done, value} = await reader.read();
  if (done) break;

  buffer += decoder.decode(value, {stream: true});
  const lines = buffer.split("\n");
  buffer = lines.pop() || "";  // Keep incomplete line

  for (const line of lines) {
    if (line.trim()) {
      const envelope = JSON.parse(line);  // {data, timestamp}
      const event = StreamEventSchema.parse(envelope.data);  // Zod validation
      onEvent(event);
    }
  }
}
```

#### Timeout Behavior
- **30-second watchdog**: Abort if no data received for 30s
- **Retry with exponential backoff**: Up to 5 retries, delays: 1s, 2s, 4s, 8s, 16s (capped at 30s)

---

## F. Error Handling & Recovery

### F.1 FactSet Data Gap Filling (Observed)

**Confidence**: HIGH (direct screenshot evidence).

#### Scenario
From screenshot 03:48:18:

> "Fill critical data gaps from FactSet failures by gathering financial, geographic, and investor data from alternative sources (SEC filings, Balance...)"

#### Recovery Process

```
1. Initial plan executes with FactSet tool
2. FactSet API returns errors/incomplete data
3. Fan-in aggregation detects gaps
4. Meta-reasoning agent analyzes:
   - What data was requested
   - What data was returned
   - What is missing
5. New plan created with fallback tools:
   - SEC filings for financial data
   - Alternative sources for geographic/investor data
6. Execute gap-filling plan
7. Merge with original results
8. Proceed to synthesis
```

**Key Insight**: System doesn't fail on tool errors — it **reasons about gaps** and **dispatches targeted recovery**.

### F.2 Retry Patterns

**Confidence**: MEDIUM (inferred from streaming timeout behavior).

#### Stream-Level Retries
From `02_streaming_protocol.md`:
- Up to 5 retries on stream drop
- Exponential backoff: 1s, 2s, 4s, 8s, 16s
- Only retries if partial data received (not on immediate failure)

#### Tool-Level Retries (Inferred)
Pattern observed in gap-filling suggests:

```python
def call_tool_with_retry(tool, max_retries=3):
    for attempt in range(max_retries):
        try:
            result = tool.execute()
            if result.is_valid():
                return result
            else:
                emit_event("node_tool_event",
                          event="invalid_result",
                          metadata={"attempt": attempt + 1})
        except Exception as e:
            emit_event("node_tool_event",
                      event="tool_error",
                      metadata={"error": str(e), "attempt": attempt + 1})
            if attempt < max_retries - 1:
                await sleep(exponential_backoff(attempt))

    # All retries failed → fallback or gap-filling
    return ToolFailure(tool_id=tool.id)
```

### F.3 Timeout Handling

**Confidence**: MEDIUM-HIGH (streaming timeout observed, tool timeouts inferred).

#### Stream Timeout
- 30-second idle timeout
- Client-side watchdog timer
- Triggers retry logic

#### Tool Timeout (Inferred)
Likely per-tool timeout with graceful handling:

```python
async def execute_tool_with_timeout(tool, timeout_seconds=60):
    try:
        result = await asyncio.wait_for(
            tool.execute(),
            timeout=timeout_seconds
        )
        return result
    except asyncio.TimeoutError:
        emit_event("node_tool_event",
                  event="tool_timeout",
                  metadata={"timeout_seconds": timeout_seconds})
        return ToolTimeout(tool_id=tool.id)
```

### F.4 Graceful Degradation Strategy

**Confidence**: MEDIUM-HIGH (observed pattern in gap-filling and event schema).

#### Degradation Levels

| Failure Scope | Degradation Strategy | User Impact |
|---------------|---------------------|-------------|
| Single tool call | Retry → Fallback → Log as partial | Footnote: "Data from alternative source" |
| Single task | Continue other tasks → Gap-filling plan | Delayed completion, possible reduced quality |
| Critical path | Request clarification from user | Interrupt with `clarification_needed` event |
| Entire run | Terminal ERROR event | User sees error message, can retry |

#### Best-Effort Completion

From screenshot evidence, system prioritizes:
1. **Completing with imperfect data** > blocking on errors
2. **Transparent fallbacks** > hiding gaps
3. **Adaptive re-planning** > fixed retry loops

**Example Flow**:
```
Initial plan (12 tasks):
  ✓ Tasks 1-8 succeed
  ✗ Tasks 9-10 fail (FactSet timeout)
  ✓ Tasks 11-12 succeed

Meta-reasoning:
  "Tasks 9-10 failed. We need financial data.
   Dispatch gap-filling with SEC filings."

Gap-filling plan (2 tasks):
  ✓ Task 13: SEC 10-K scraping
  ✓ Task 14: Alternative investor data

Synthesis:
  Combines 8 + 2 + 2 + 2 = 14 successful results
  Proceeds to deliverable generation
```

---

## G. Additional Findings

### G.1 Human-in-the-Loop (Browser Use)

**Confidence**: HIGH (direct evidence from streaming events).

From event schema:

```typescript
type BrowserUseStart = {
  type: "browser_use_start";
  browser_session_id: string;
  browser_stream_url: string;  // Separate websocket/SSE stream
  timestamp: number;
}

type BrowserUseAwaitUserInput = {
  type: "browser_use_await_user_input";
  browser_session_id: string;
  agent_message?: string;  // Prompt for user action
}
```

**Completion Endpoint**: `/api/browser-use/user-input-done`

**Pattern**: Agent can pause mid-execution, wait for user interaction in a browser session, then resume.

**Use Cases** (inferred):
- OAuth authentication flows
- CAPTCHA solving
- Manual data entry/verification
- Interactive data exploration

### G.2 Async Entity Generation

**Confidence**: MEDIUM (event schema indicates pattern).

From `done` event:

```typescript
type DoneEvent = {
  type: "done";
  message?: Message;
  has_async_entities_pending?: boolean;  // ← Key flag
}
```

**Implication**: Deliverable generation can continue **in the background** after the main message completes.

**Pattern**:
```
1. Research completes → done(has_async_entities_pending: true)
2. User sees message with "Generating report..." indicator
3. Background: v0.app generates report
4. Periodic polling: GET /api/website/status/{entity_id}
5. On completion: entity.created event (or polling returns ready)
6. UI updates with download link
```

### G.3 Durable Replay

**Confidence**: HIGH (direct code evidence).

From `02_streaming_protocol.md`:

> "ReplayStream mode: iterates over a stored list of prior events, calls the same onEvent handler, and synthesizes a terminal done with the stored final message."

**Implementation** (from bundle analysis):

```typescript
class ReplayStream {
  async replay(storedEvents: StreamEvent[]) {
    for (const event of storedEvents) {
      if (event.type === "heartbeat") continue;  // Skip in replay
      await this.onEvent(event);
      await sleep(REPLAY_DELAY);  // Throttle playback
    }

    // Synthesize terminal event
    this.onEvent({
      type: "done",
      message: this.finalMessage,
      timestamp: Date.now()
    });
  }
}
```

**Use Cases**:
- Chat history view (replay old runs)
- Debugging (inspect event sequence)
- Resume from checkpoint (partial replay)

### G.4 Multi-Stage Report Generation

**Confidence**: HIGH (direct evidence from generation pipeline analysis).

From `GENERATION_PIPELINE.md`, Report A git history:

```
Phase 1: Standards Agent
  - Creates chart-styles.ts
  - Writes conventions doc
  - ~100 seconds

Phase 2: Parallel Feature Agents (4 agents)
  Agent A: LineChartDemo, BarChartDemo
  Agent B: ScatterPlotDemo, HistogramDemo
  Agent C: HeatmapDemo
  Agent D: SankeyDemo, TreemapDemo
  - Each completes in ~100 seconds
  - Work on isolated feature branches

Phase 3: Integration Agent
  - Merges all branches
  - Fixes type errors
  - Validates build
  - ~100 seconds
```

**Pattern**: Standards → Parallel → Integration (classic map-reduce for code generation).

**Total Time**: ~300 seconds for complex report generation.

---

## H. Confidence Summary by Section

| Section | Finding | Confidence |
|---------|---------|------------|
| A.1 Primary Orchestration Agents | Planner, Executor, Synthesizer, Generator roles | **HIGH** |
| A.2 Specialized Sub-Agents | Report generation agents (Standards, Feature, Integration) | **HIGH** |
| A.3 Agent Identification | Hierarchical IDs in events | **MEDIUM** |
| B.1 Plan Decomposition | Data model and linked-list structure | **HIGH** |
| B.2 Parallel Dispatch | Fan-out pattern with Send() | **MEDIUM-HIGH** |
| B.3 Task Lifecycle | State machine and transitions | **HIGH** |
| B.4 Fan-In Aggregation | Data brief creation | **MEDIUM** |
| B.5 Meta-Reasoning | Adaptive re-planning | **MEDIUM-HIGH** |
| C.1 Tool Categories | Specific tool names | **MEDIUM-LOW** |
| C.2 Tool Return Schemas | Entity/citation structure | **MEDIUM** |
| C.3 Tool Execution Flow | Event sequence | **MEDIUM-HIGH** |
| C.4 Tool Failure Handling | Fallback cascade | **MEDIUM** |
| D.1 Data Flow | Planner → Executors → Synthesizer → Generator | **MEDIUM** |
| D.2 State Persistence | Event sourcing | **HIGH** |
| D.3 Intermediate Storage | Entity abstraction | **MEDIUM-HIGH** |
| D.4 Error Propagation | Levels and handling | **MEDIUM** |
| E.1 Event Taxonomy | 22 event types | **HIGH** |
| E.2 Event Ordering | Temporal and causal ordering | **MEDIUM-HIGH** |
| E.3 Event-to-UI Mapping | Component updates | **HIGH** |
| E.4 Stream Transport | NDJSON over fetch | **HIGH** |
| F.1 FactSet Gap Filling | Observed recovery pattern | **HIGH** |
| F.2 Retry Patterns | Stream retries (observed), tool retries (inferred) | **MEDIUM-HIGH** |
| F.3 Timeout Handling | Stream timeout (observed), tool timeout (inferred) | **MEDIUM-HIGH** |
| F.4 Graceful Degradation | Best-effort completion | **MEDIUM-HIGH** |
| G.1 Browser Use | Human-in-the-loop pattern | **HIGH** |
| G.2 Async Entity Generation | Background deliverable creation | **MEDIUM** |
| G.3 Durable Replay | Event replay mechanism | **HIGH** |
| G.4 Multi-Stage Report Gen | Standards → Parallel → Integration | **HIGH** |

---

## I. Open Questions

1. **What LLM powers the planner?** (GPT-4, Claude, proprietary?)
2. **Is the meta-reasoning agent the same as the planner, or a separate model?**
3. **What triggers the choice between single-stage vs multi-stage planning?**
4. **How are tool credentials managed?** (per-user, per-workspace, system-wide?)
5. **What is the exact relationship between Superagent and v0.app?** (Same company? API integration? Shared infrastructure?)
6. **Are there rate limits per tool/provider?** (How are they enforced?)
7. **What happens if ALL fallback tools fail?** (Does it trigger clarification_needed or proceed with gaps?)
8. **Is the 14-task limit in the screenshot an upper bound, or does it scale?**
9. **How does billing work for premium sources?** (per-call, monthly, credits?)
10. **What is the "Balance..." truncated source in the gap-filling screenshot?**

---

## J. References

### Primary Sources (This Analysis)
- `/Users/markforster/AirTable-SuperAgent/reports/03_agent_orchestration_inference.md`
- `/Users/markforster/AirTable-SuperAgent/reports/02_streaming_protocol.md`
- `/Users/markforster/AirTable-SuperAgent/docs/JS_DESIGN_TOKENS.md`
- `/Users/markforster/AirTable-SuperAgent/docs/SCREENSHOT_ANALYSIS.md`
- `/Users/markforster/AirTable-SuperAgent/docs/GENERATION_PIPELINE.md`
- `/Users/markforster/AirTable-SuperAgent/docs/SUPERAGENT_REPORT_ANALYSIS.md`
- `/Users/markforster/AirTable-SuperAgent/docs/plans/2026-02-16-MAPPING-01-RESEARCH-ORCHESTRATOR.md`

### External References
- **Airtable Newsroom**: "Announcing the launch of Superagent" (Jan 27, 2026)
  https://www.airtable.com/newsroom/announcing-the-launch-of-superagent
- **LangGraph Documentation**: Persistence, Human-in-the-Loop, Streaming
  https://langchain-ai.github.io/langgraphjs/concepts/

### Bundle Evidence
- `774-e1971e2500ea1c79.js` — Streaming protocol Zod schemas (22 event types)
- `[chatId]-058fa92e7cc73e56.js` — Chat page orchestration code
- `1889-c64cad4788e7b7b9.js` — Chart components and GML rendering

### Screenshot Evidence
- Screenshot 03:31:40 — Task cards 20-23 with `PROCESSING` and `CREATING NOTES` states
- Screenshot 03:48:18 — Activity panel showing 14 parallel workstreams and meta-reasoning

---

**End of Analysis**

*Generated: 2026-02-16*
*Total Evidence Sources: 10 documents, 3 JS bundles, 2 screenshots, 2 external references*
